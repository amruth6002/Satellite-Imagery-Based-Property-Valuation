{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Satellite Imagery-Based Property Valuation: High-Performance Model\n",
                "\n",
                "This notebook implements the complete pipeline:\n",
                "1.  **Tabular Baseline:** XGBoost on ALL features (Including Lat/Long).\n",
                "2.  **Multimodal Network:** ResNet18 + MLP (with Location Features).\n",
                "3.  **Visual Forensics:** Grad-CAM Analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import cv2\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torchvision.transforms as transforms\n",
                "import torchvision.models as models\n",
                "\n",
                "# Suppress warnings\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "# Check device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data & Baseline Model\n",
                "**Performance Boost:** We are now including `Lat`, `Long`, and `Zipcode`. Location is the most important factor in real estate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load csv\n",
                "DATA_PATH = '../data/train.csv' \n",
                "if not os.path.exists(DATA_PATH):\n",
                "    DATA_PATH = 'data/train.csv' # Fallback for local run\n",
                "\n",
                "df = pd.read_csv(DATA_PATH)\n",
                "print(f\"Loaded {len(df)} rows\")\n",
                "\n",
                "# --- FEATURE ENGINEERING ---\n",
                "# 1. Handle Date\n",
                "df['date'] = pd.to_datetime(df['date'])\n",
                "df['year'] = df['date'].dt.year\n",
                "df['month'] = df['date'].dt.month\n",
                "\n",
                "# 2. Select Features (Include Lat/Long this time!)\n",
                "# We only drop ID and PRICE. Everything else is useful.\n",
                "exclude_cols = ['id', 'price', 'date'] \n",
                "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
                "print(f\"Training on {len(feature_cols)} features: {feature_cols}\")\n",
                "\n",
                "X = df[feature_cols].fillna(0)\n",
                "y = df['price']\n",
                "\n",
                "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train Baseline (XGBoost/GBR)\n",
                "print(\"Training Tabular Baseline (with Location features)...\")\n",
                "baseline_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=5, random_state=42)\n",
                "baseline_model.fit(X_train, y_train)\n",
                "\n",
                "# Evaluate Baseline\n",
                "preds = baseline_model.predict(X_val)\n",
                "baseline_rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
                "baseline_r2 = r2_score(y_val, preds)\n",
                "\n",
                "print(f\"Tabular Baseline RMSE: ${baseline_rmse:,.2f}\")\n",
                "print(f\"Tabular Baseline R2 Score: {baseline_r2:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Multimodal Deep Learning\n",
                "Training on Log(Price) for stability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset & Transforms\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "class RealEstateDataset(Dataset):\n",
                "    def __init__(self, tabular_data, image_dir, feature_cols, scaler=None, transform=None):\n",
                "        self.data = tabular_data.reset_index(drop=True)\n",
                "        self.image_dir = image_dir\n",
                "        self.transform = transform\n",
                "        self.feature_cols = feature_cols\n",
                "        self.num_features = len(self.feature_cols)\n",
                "        \n",
                "        if scaler:\n",
                "            self.X_scaled = scaler.transform(self.data[self.feature_cols].fillna(0))\n",
                "        else:\n",
                "            self.X_scaled = self.data[self.feature_cols].fillna(0).values\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        features = torch.tensor(self.X_scaled[idx].astype(np.float32))\n",
                "        \n",
                "        # Log-Price Target\n",
                "        raw_price = self.data.loc[idx, 'price']\n",
                "        log_price = np.log1p(raw_price)\n",
                "        target = torch.tensor(log_price, dtype=torch.float32)\n",
                "        \n",
                "        img_id = str(int(self.data.loc[idx, 'id']))\n",
                "        img_path = os.path.join(self.image_dir, f\"{img_id}.jpg\")\n",
                "        \n",
                "        try:\n",
                "            image = Image.open(img_path).convert('RGB')\n",
                "        except:\n",
                "            image = Image.new('RGB', (224, 224), color='black')\n",
                "            \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "            \n",
                "        return image, features, target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Architecture\n",
                "class MultimodalNet(nn.Module):\n",
                "    def __init__(self, num_tabular_features):\n",
                "        super(MultimodalNet, self).__init__()\n",
                "        self.cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
                "        self.cnn.fc = nn.Identity()\n",
                "        \n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(num_tabular_features, 128), # Wider for more features\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 64),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "        \n",
                "        self.fusion = nn.Sequential(\n",
                "            nn.Linear(512 + 64, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(256),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(256, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, image, tabular):\n",
                "        x_img = self.cnn(image)\n",
                "        x_tab = self.mlp(tabular)\n",
                "        combined = torch.cat((x_img, x_tab), dim=1)\n",
                "        return self.fusion(combined)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop & Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_and_evaluate(epochs=15):\n",
                "    # Configuration\n",
                "    IMG_DIR = '../data/images' \n",
                "    if not os.path.exists(IMG_DIR): IMG_DIR = 'data/images'\n",
                "        \n",
                "    train_split, val_split = train_test_split(df, test_size=0.2, random_state=42)\n",
                "\n",
                "    scaler = StandardScaler()\n",
                "    scaler.fit(train_split[feature_cols].fillna(0))\n",
                "    \n",
                "    train_dataset = RealEstateDataset(train_split, IMG_DIR, feature_cols, scaler, transform=transform)\n",
                "    val_dataset = RealEstateDataset(val_split, IMG_DIR, feature_cols, scaler, transform=transform)\n",
                "    \n",
                "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
                "    \n",
                "    model = MultimodalNet(num_tabular_features=train_dataset.num_features).to(device)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
                "    criterion = nn.MSELoss()\n",
                "\n",
                "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n",
                "    \n",
                "    print(f\"Starting Training on {len(train_dataset)} samples...\")\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        running_loss = 0.0\n",
                "        for images, tabs, log_prices in tqdm(train_loader):\n",
                "            images = images.to(device)\n",
                "            tabs = tabs.to(device)\n",
                "            log_prices = log_prices.to(device).unsqueeze(1)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(images, tabs)\n",
                "            loss = criterion(outputs, log_prices)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            running_loss += loss.item()\n",
                "            \n",
                "        avg_loss = running_loss/len(train_loader)\n",
                "        print(f\"Epoch {epoch+1}/{epochs}, Loss (Log-MSE): {avg_loss:.4f}\")\n",
                "        scheduler.step(avg_loss)\n",
                "        \n",
                "    print(\"\\nEvaluating on Validation Set...\")\n",
                "    model.eval()\n",
                "    all_preds_log = []\n",
                "    all_targets_log = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, tabs, log_prices in val_loader:\n",
                "            images = images.to(device)\n",
                "            tabs = tabs.to(device)\n",
                "            \n",
                "            outputs = model(images, tabs)\n",
                "            all_preds_log.extend(outputs.cpu().numpy())\n",
                "            all_targets_log.extend(log_prices.numpy())\n",
                "            \n",
                "    pred_prices = np.expm1(np.array(all_preds_log).flatten())\n",
                "    real_prices = np.expm1(np.array(all_targets_log).flatten())\n",
                "    \n",
                "    dl_rmse = np.sqrt(mean_squared_error(real_prices, pred_prices))\n",
                "    dl_r2 = r2_score(real_prices, pred_prices)\n",
                "    \n",
                "    print(\"=\"*40)\n",
                "    print(f\"Multimodal Model RMSE: ${dl_rmse:,.2f}\")\n",
                "    print(f\"Multimodal Model R2:   {dl_r2:.4f}\")\n",
                "    print(\"=\"*40)\n",
                "    \n",
                "    return model\n",
                "\n",
                "# Run Training\n",
                "trained_model = train_and_evaluate(epochs=15)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visual Analysis (Grad-CAM)\n",
                "This section visualizes **which parts of the image** the model looks at."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_visual_features(model, img_path):\n",
                "    img_pil = Image.open(img_path).convert('RGB')\n",
                "    img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
                "    \n",
                "    activations = []\n",
                "    gradients = []\n",
                "    \n",
                "    def hook_forward(module, input, output):\n",
                "        activations.append(output)\n",
                "    def hook_backward(module, grad_in, grad_out):\n",
                "        gradients.append(grad_out[0])\n",
                "        \n",
                "    handle_f = model.cnn.layer4[-1].register_forward_hook(hook_forward)\n",
                "    handle_b = model.cnn.layer4[-1].register_full_backward_hook(hook_backward)\n",
                "    \n",
                "    model.eval()\n",
                "    # Dynamic Size Fix\n",
                "    num_input_features = model.mlp[0].in_features\n",
                "    dummy_tab = torch.zeros((1, num_input_features)).to(device)\n",
                "    \n",
                "    output = model(img_tensor, dummy_tab)\n",
                "    model.zero_grad()\n",
                "    output.backward()\n",
                "    \n",
                "    pooled_gradients = torch.mean(gradients[0], dim=[0, 2, 3])\n",
                "    activation = activations[0][0]\n",
                "    for i in range(512):\n",
                "        activation[i, :, :] *= pooled_gradients[i]\n",
                "        \n",
                "    heatmap = torch.mean(activation, dim=0).cpu().detach().numpy()\n",
                "    heatmap = np.maximum(heatmap, 0)\n",
                "    heatmap /= np.max(heatmap)\n",
                "    \n",
                "    heatmap = cv2.resize(heatmap, (224, 224))\n",
                "    heatmap = np.uint8(255 * heatmap)\n",
                "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
                "    \n",
                "    original_img = cv2.cvtColor(np.array(img_pil.resize((224, 224))), cv2.COLOR_RGB2BGR)\n",
                "    superimposed = cv2.addWeighted(original_img, 0.6, heatmap, 0.4, 0)\n",
                "    \n",
                "    plt.figure(figsize=(10, 5))\n",
                "    plt.subplot(1, 2, 1); plt.title(\"Original\"); plt.imshow(img_pil)\n",
                "    plt.subplot(1, 2, 2); plt.title(\"Model Attention\"); plt.imshow(cv2.cvtColor(superimposed, cv2.COLOR_BGR2RGB))\n",
                "    plt.show()\n",
                "    \n",
                "    handle_f.remove()\n",
                "    handle_b.remove()\n",
                "\n",
                "# Example Usage:\n",
                "# analyze_visual_features(trained_model, '../data/images/some_id.jpg')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}